{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python script to run the pre-processed learning curves for a single step of GD\n",
    "import numpy as np\n",
    "from scipy.special import erf   \n",
    "import os \n",
    "import time "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Useful functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we investigate the generalization performance after one step for two-neuron teachers ($k=2$). \n",
    "\n",
    "Let us define some useful functions that we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few lines to define relevant functions \n",
    "def perpendicular_vector(v):\n",
    "    d = len(v)\n",
    "    w = np.zeros(d)\n",
    "    w[0] = np.random.randn()  \n",
    "    w -= np.dot(w, v) * v / np.dot(v, v) \n",
    "    w /= np.linalg.norm(w)\n",
    "    return w\n",
    "def sample_data(n,ntest,d):\n",
    "    # function to sample data \n",
    "    Z = np.random.randn(n,d) ; Ztest = np.random.randn(ntest,d)\n",
    "    Y =  (f01(Z@theta1) + f02(Z@theta2)) / 2  + np.sqrt(noise)*np.random.randn(n)\n",
    "    Ytest =  (f01(Ztest@theta1) + f02(Ztest@theta2)) / 2 + np.sqrt(noise)*np.random.randn(ntest)\n",
    "    return Z,Ztest,Y,Ytest\n",
    "def ridge_estimator(X, y, lamb=0.1):\n",
    "    # Implements the pseudo-inverse ridge estimator.\n",
    "    m, n = X.shape\n",
    "    if m >= n:\n",
    "        return np.linalg.inv(X.T @ X + lamb*np.identity(n)) @ X.T @ y\n",
    "    elif m < n:\n",
    "        return X.T @ np.linalg.inv(X @ X.T + lamb*np.identity(m)) @ y\n",
    "def get_errors_ridge(Xtrain,Xtest,Ytrain,Ytest,lamb,flag = True):\n",
    "    ' get errors for ridge regression with fixed data matrices'\n",
    "    ' normalize the data by dividing by sqrt(p) in the ridge estimator while label are of O(1) already'\n",
    "    n,p = Xtrain.shape\n",
    "    eg, et = [], [] \n",
    "    # Iterate over different realisations of the problem.\n",
    "    w = ridge_estimator(Xtrain / np.sqrt(p), Ytrain, lamb)\n",
    "    yhat_train = Xtrain @ w / np.sqrt(p)\n",
    "    yhat_test = Xtest @ w   / np.sqrt(p)\n",
    "    # Train loss\n",
    "    train_loss = np.mean((Ytrain - yhat_train)**2)\n",
    "    # Fresh samples\n",
    "    test_error = np.mean((Ytest - yhat_test)**2) \n",
    "    eg.append(test_error)   ;  et.append(train_loss)\n",
    "    if flag:\n",
    "        print(f' we have train loss {train_loss} and test error {test_error}')\n",
    "    # Return average and standard deviation of both errors\n",
    "    return (np.mean(et), np.mean(eg) , np.std(et), \n",
    "             np.std(eg),w)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter setup "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We fix the relevant dimensions $(p,d)$\n",
    "- We sample the orthonormal teacher vectors $\\vec{w}^*_1,\\vec{w}^*_2$. \n",
    "- We fix the first and second layer at initialization $W_0,\\vec{a}_0$.\n",
    "- We define the different activations we may consider in Teacher-Student setup.\n",
    "- We build the array of sample sizes $\\vec{n_s}$ from which we compute the generalization error curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of possible student activation functions and its derivatives \n",
    "stud_acts = { 'relu': lambda x: np.maximum(x,0),  'hermite1+2+4_norm': lambda x: (x**4 - 6*x**2 + 3) /24 + (x**2 -1) /2 + x}\n",
    "stud_ders = { 'relu': lambda x: (x>0).astype(int), 'hermite1+2+4_norm': lambda x: x**3 - 3*x}\n",
    "# Dictionary of possible teacher activation functions\n",
    "teach_acts = {'erf': lambda x: erf(x),'hermite1+2+4_norm': lambda x: (x**4 - 6*x**2 + 3) /24 + (x**2 -1) /2 + x}\n",
    "stud_act = 'relu' \n",
    "f = stud_acts[stud_act]\n",
    "fprime = stud_ders[stud_act] \n",
    "fnn0 = lambda D , W , a : 1/np.sqrt(p)*f(D@W.T)@a\n",
    "teach_act1 = 'hermite1+2+4_norm'   ; f01 = teach_acts[teach_act1]\n",
    "teach_act2 = 'hermite1+2+4_norm'    ; f02 = teach_acts[teach_act2]\n",
    "number_ns = 3 \n",
    "d = 512 \n",
    "ntest = int(1e5) ; lamb = 1 ; noise = 0 \n",
    "exp_min_n = 1.3 ; exp_max_n = 2\n",
    "p = 1024\n",
    "# choose sample size array \n",
    "ns = np.logspace(exp_min_n,exp_max_n,number_ns,base=d,dtype=int) # Take the range for th eplot of log-normalized sample complexity\n",
    "# Initialize the weights\n",
    "W0 = 1/np.sqrt(d)*np.random.randn(p,d)\n",
    "# sample the orthonormal teacher vectors -  choose k=2 for simplicity\n",
    "v1 = np.random.randn(d) \n",
    "theta1 = v1/np.linalg.norm(v1)\n",
    "v2 = perpendicular_vector(v1)\n",
    "theta2 = v2/np.linalg.norm(v2)\n",
    "# sample second layer and fix it \n",
    "a0 = 1/np.sqrt(p)*np.random.randn(p) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning curve construction "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement the GD training protocol with a preprocessing step. The preprocessing step is crucial in order to learn in one giant step of GD. Indeed, as Theorem 1 provably states, it is not possible to get fully specialized hidden student units with one giant step of GD in the $n = \\mathcal{O}(d^l)$ regime, if directions associated to teacher Hermite coefficients lower than $l$ are not suppressed, or equivalently, if the leap index of the problem is lower than $l$. In the following we consider sample sizes up to $n = \\mathcal{O}(d^2)$, and a teacher function with leap index $l=1$. Therefore, we need to remove an estimate of the first teacher hermite coefficient, in order to obtain a problem with effective leap index $l=2$.  \n",
    "\n",
    "- We iterate over the value in $\\vec{n_s}$ \n",
    "- For each sample we set the learning rate adaptively to have $\\eta = \\mathcal{O}(p\\sqrt{\\frac{n}{d}})$\n",
    "- We average over 10 random draws \n",
    "- We use standard deviation to give confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START  --- for regime exponent 1.2999524948096965 d =512 and p=1024  \n",
      "FINISH --- for regime exponent & n,d = 1.2999524948096965 & (3326, 512) the gen error is 0.3360002036384977, the std is 0.0063229646780440935 and it took t=41.65862417221069\n",
      "START  --- for regime exponent 1.6499990492421464 d =512 and p=1024  \n",
      "FINISH --- for regime exponent & n,d = 1.6499990492421464 & (29532, 512) the gen error is 0.2067522390105553, the std is 0.005893831748022804 and it took t=77.16522908210754\n",
      "START  --- for regime exponent 2.0 d =512 and p=1024  \n",
      "FINISH --- for regime exponent & n,d = 2.0 & (262144, 512) the gen error is 0.07741778601141551, the std is 0.002084362460098963 and it took t=445.02054595947266\n"
     ]
    }
   ],
   "source": [
    "# iterate over array of sample sizes\n",
    "for j,n in enumerate(ns):\n",
    "    start = time.time()\n",
    "    # set the (giant) learning rate adaptively with the current sample size\n",
    "    eta = 10*np.sqrt(n)*np.sqrt(p)\n",
    "    print(f'START  --- for regime exponent {np.log(n)/np.log(d)} d ={d} and p={p}  ')\n",
    "    # GD on the RF weights \n",
    "    nseeds = 10 ;  errgs = []\n",
    "    for seed in range(nseeds):\n",
    "        Z,Ztest,Y,Ytest = sample_data(n, ntest, d)\n",
    "        # preprocess data removing the estimation of the first hermite coefficient\n",
    "        A = np.mean(Y) ; A_test = np.mean(Ytest)\n",
    "        B = np.mean(Y.reshape(-1,1)*Z,axis=0) ; B_test = np.mean(Ytest.reshape(-1,1)*Ztest,axis=0)\n",
    "        Y_touse = Y - A - Z@B ; Ytest_touse = Ytest - A_test - Ztest@B_test\n",
    "        # compute gradient \n",
    "        G = 1/n * Z.T @ (1/np.sqrt(p)*np.outer( ( Y_touse - fnn0(Z,W0,a0) ) , a0) * fprime(Z@W0.T))\n",
    "        Wgd = W0 + eta*G.T\n",
    "        # generate features \n",
    "        X = f(Z@Wgd.T) ; Xtest = f(Ztest@Wgd.T)\n",
    "        # compute ridge estimator \n",
    "        e1,e2,s1,s2,w = get_errors_ridge(X,Xtest,Y_touse,Ytest_touse,lamb,flag=False)\n",
    "        # compute predictions injecting back the preprocessed part\n",
    "        Yhat = A + Z@B + 1/np.sqrt(p)*X@w\n",
    "        Yhat_test = A_test + Ztest@B_test + 1/np.sqrt(p)*Xtest@w\n",
    "        errgs.append(np.mean((Yhat_test - Ytest)**2))\n",
    "    test_error = np.mean(errgs) ; test_error_std = np.std(errgs)\n",
    "    end = time.time()\n",
    "    print(f'FINISH --- for regime exponent & n,d = {np.log(n)/np.log(d)} & {n,d} the gen error is {test_error}, the std is {test_error_std} and it took t={end - start}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
